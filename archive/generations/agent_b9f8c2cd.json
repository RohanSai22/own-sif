{
  "agent_id": "agent_b9f8c2cd",
  "generation": 0,
  "parent_id": null,
  "created_at": "2025-06-26T07:36:43.884155",
  "performance_score": 0.0,
  "success_rate": 0.0,
  "total_tasks": 0,
  "task_results": [],
  "mutations_applied": [],
  "source_code": {
    "agent/agent_core.py": "\"\"\"Core agent implementation for Prometheus 2.0 - The self-improving AI agent.\"\"\"\n\nimport json\nimport os\nimport time\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nimport logging\n\nfrom llm_provider.unified_client import llm_client, LLMResponse\nfrom tools.tool_manager import ToolManager\nfrom agent.prompts import (\n    SYSTEM_PROMPT, REFLECTION_PROMPT, PROBLEM_SOLVING_PROMPT,\n    CODE_ANALYSIS_PROMPT, IMPROVEMENT_PROMPT\n)\nfrom framework.tui import tui\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of solving a task.\"\"\"\n    task_id: str\n    success: bool\n    score: float\n    solution: str\n    execution_time: float\n    errors: List[str]\n    tools_used: List[str]\n    reasoning: str\n\nclass PrometheusAgent:\n    \"\"\"The core self-improving AI agent.\"\"\"\n    \n    def __init__(self, agent_id: str = None, project_root: str = \".\"):\n        self.agent_id = agent_id or f\"agent_{uuid.uuid4().hex[:8]}\"\n        self.project_root = project_root\n        self.generation = 0\n        self.parent_id = None\n        \n        # Initialize components\n        self.tool_manager = ToolManager(project_root)\n        self.llm_client = llm_client\n        \n        # Performance tracking\n        self.task_results: List[TaskResult] = []\n        self.total_score = 0.0\n        self.success_rate = 0.0\n        \n        # Pattern tracking for autonomous tool creation (Pillar 1)\n        self.action_patterns: Dict[str, List[Dict[str, Any]]] = {}\n        self.tool_usage_history: List[Dict[str, Any]] = []\n        self.repetitive_patterns: List[Dict[str, Any]] = []\n        \n        # Knowledge base integration (Pillar 3)\n        self.knowledge_base_path = os.path.join(project_root, \"archive\", \"knowledge_base.md\")\n        self.stored_knowledge: Dict[str, str] = {}\n        \n        # Load existing knowledge\n        self._load_knowledge_base()\n        \n        # Conversation history for context\n        self.conversation_history: List[Dict[str, str]] = []\n        \n        logger.info(f\"Initialized agent {self.agent_id}\")\n    \n    def solve_task(self, task: Dict[str, Any]) -> TaskResult:\n        \"\"\"\n        Solve a single SWE-bench task.\n        \n        Args:\n            task: Task dictionary with problem description and metadata\n            \n        Returns:\n            TaskResult with the solution and performance metrics\n        \"\"\"\n        start_time = time.time()\n        task_id = task.get(\"instance_id\", f\"task_{uuid.uuid4().hex[:8]}\")\n        \n        tui.log_thought(f\"Starting task: {task_id}\")\n        tui.update_task(f\"Solving task {task_id}\")\n        \n        try:\n            # Extract task information\n            problem_statement = task.get(\"problem_statement\", \"\")\n            repo_name = task.get(\"repo\", \"\")\n            base_commit = task.get(\"base_commit\", \"\")\n            \n            # Get available tools\n            available_tools = self._format_available_tools()\n            \n            # Create the problem-solving prompt\n            prompt = PROBLEM_SOLVING_PROMPT.format(\n                problem_statement=problem_statement,\n                repo_name=repo_name,\n                base_commit=base_commit,\n                available_tools=available_tools\n            )\n            \n            tui.log_thought(\"Analyzing problem and planning approach...\")\n            \n            # Generate solution using LLM\n            solution_response = self._generate_response([\n                {\"role\": \"user\", \"content\": prompt}\n            ])\n            \n            # Execute the solution\n            execution_result = self._execute_solution(solution_response.content, task)\n            \n            execution_time = time.time() - start_time\n            \n            # Track action patterns for tool creation opportunities (Pillar 1)\n            self._track_action_pattern(\"problem_solving\", {\n                \"target\": f\"{repo_name}_analysis\",\n                \"problem_type\": self._categorize_problem(problem_statement),\n                \"tools_used\": execution_result.get(\"tools_used\", []),\n                \"complexity\": len(execution_result.get(\"tools_used\", [])),\n                \"success\": execution_result[\"success\"]\n            })\n            \n            # Calculate score based on success and execution quality\n            score = self._calculate_task_score(execution_result, execution_time)\n            \n            # Create task result\n            result = TaskResult(\n                task_id=task_id,\n                success=execution_result[\"success\"],\n                score=score,\n                solution=solution_response.content,\n                execution_time=execution_time,\n                errors=execution_result.get(\"errors\", []),\n                tools_used=execution_result.get(\"tools_used\", []),\n                reasoning=execution_result.get(\"reasoning\", \"\")\n            )\n            \n            # Update performance tracking\n            self.task_results.append(result)\n            self._update_performance_metrics()\n            \n            # Log result\n            if result.success:\n                tui.log_action(\"Task\", f\"\u2713 Completed {task_id} (score: {score:.3f})\", \"SUCCESS\")\n            else:\n                tui.log_action(\"Task\", f\"\u2717 Failed {task_id} (score: {score:.3f})\", \"ERROR\")\n            \n            return result\n            \n        except Exception as e:\n            execution_time = time.time() - start_time\n            error_msg = str(e)\n            \n            logger.error(f\"Error solving task {task_id}: {error_msg}\")\n            tui.log_action(\"Task\", f\"\u2717 Error in {task_id}: {error_msg}\", \"ERROR\")\n            \n            # Use web search to research error-specific solutions\n            error_research = self._research_error_solutions(error_msg, task)\n            \n            # Try to recover with the research information\n            if error_research:\n                tui.log_thought(\"Attempting error recovery with research insights...\")\n                try:\n                    # Create recovery prompt with error research\n                    recovery_prompt = f\"\"\"\nORIGINAL TASK:\n{task.get(\"problem_statement\", \"\")}\n\nENCOUNTERED ERROR:\n{error_msg}\n\nRESEARCH FINDINGS:\n{error_research}\n\nBased on this error and research, please provide a corrected approach to solve the original task. Focus specifically on avoiding the error that occurred.\n\"\"\"\n                    \n                    recovery_response = self._generate_response([\n                        {\"role\": \"user\", \"content\": recovery_prompt}\n                    ])\n                    \n                    # Execute recovery solution\n                    recovery_result = self._execute_solution(recovery_response.content, task)\n                    \n                    if recovery_result[\"success\"]:\n                        execution_time = time.time() - start_time\n                        score = self._calculate_task_score(recovery_result, execution_time)\n                        \n                        result = TaskResult(\n                            task_id=task_id,\n                            success=True,\n                            score=score,\n                            solution=recovery_response.content,\n                            execution_time=execution_time,\n                            errors=[error_msg],  # Keep original error for learning\n                            tools_used=recovery_result.get(\"tools_used\", []) + [\"web_search\"],\n                            reasoning=f\"Recovered from error using web research: {recovery_result.get('reasoning', '')}\"\n                        )\n                        \n                        self.task_results.append(result)\n                        self._update_performance_metrics()\n                        \n                        tui.log_action(\"Task\", f\"\u2713 Recovered {task_id} (score: {score:.3f})\", \"SUCCESS\")\n                        return result\n                        \n                except Exception as recovery_error:\n                    logger.warning(f\"Recovery attempt failed: {recovery_error}\")\n            \n            # Return failed result\n            result = TaskResult(\n                task_id=task_id,\n                success=False,\n                score=0.0,\n                solution=\"\",\n                execution_time=execution_time,\n                errors=[error_msg],\n                tools_used=[],\n                reasoning=f\"Task failed with error: {error_msg}\"\n            )\n            \n            self.task_results.append(result)\n            self._update_performance_metrics()\n            \n            return result\n    \n    def _execute_solution(self, solution: str, task: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute the generated solution.\"\"\"\n        execution_result = {\n            \"success\": False,\n            \"errors\": [],\n            \"tools_used\": [],\n            \"reasoning\": \"\",\n            \"files_modified\": []\n        }\n        \n        try:\n            # Parse the solution to extract tool calls and reasoning\n            reasoning_parts = []\n            tools_used = []\n            \n            # This is a simplified execution - in practice, you might want\n            # to parse and execute specific tool calls from the solution\n            \n            # For now, we'll simulate execution based on solution content\n            if \"web_search\" in solution.lower():\n                tui.log_action(\"web_search\", \"Searching for relevant information\", \"TOOL\")\n                tools_used.append(\"web_search\")\n                \n            if \"read_file\" in solution.lower():\n                tui.log_action(\"read_file\", \"Reading repository files\", \"TOOL\")\n                tools_used.append(\"read_file\")\n                \n            if \"write_file\" in solution.lower():\n                tui.log_action(\"write_file\", \"Writing solution files\", \"TOOL\")\n                tools_used.append(\"write_file\")\n                \n            # Basic success criteria\n            has_code_changes = any(keyword in solution.lower() for keyword in \n                                 [\"write_file\", \"modify\", \"patch\", \"fix\", \"implement\"])\n            has_reasoning = len(solution) > 100\n            \n            execution_result.update({\n                \"success\": has_code_changes and has_reasoning,\n                \"tools_used\": tools_used,\n                \"reasoning\": solution[:500] + \"...\" if len(solution) > 500 else solution\n            })\n            \n        except Exception as e:\n            execution_result[\"errors\"].append(str(e))\n        \n        return execution_result\n    \n    def _calculate_task_score(self, execution_result: Dict[str, Any], execution_time: float) -> float:\n        \"\"\"Calculate a score for the task based on execution results.\"\"\"\n        score = 0.0\n        \n        # Base score for success\n        if execution_result[\"success\"]:\n            score += 0.5\n        \n        # Bonus for using tools effectively\n        tools_used = execution_result.get(\"tools_used\", [])\n        if tools_used:\n            score += min(0.2, len(tools_used) * 0.05)\n        \n        # Bonus for detailed reasoning\n        reasoning = execution_result.get(\"reasoning\", \"\")\n        if len(reasoning) > 200:\n            score += 0.1\n        \n        # Penalty for errors\n        errors = execution_result.get(\"errors\", [])\n        score -= len(errors) * 0.1\n        \n        # Time efficiency bonus/penalty\n        if execution_time < 60:  # Under 1 minute\n            score += 0.1\n        elif execution_time > 300:  # Over 5 minutes\n            score -= 0.1\n        \n        # Ensure score is between 0 and 1\n        return max(0.0, min(1.0, score))\n    \n    def self_reflect_and_improve(self, source_code_dict: Dict[str, str], performance_logs: str) -> str:\n        \"\"\"\n        Enhanced self-reflection with autonomous tool creation, self-modification, and curiosity-driven exploration.\n        \n        Args:\n            source_code_dict: Dictionary of agent source code files\n            performance_logs: String containing performance analysis\n            \n        Returns:\n            JSON string with proposed improvements, new tools, and exploration queries\n        \"\"\"\n        tui.log_thought(\"Beginning enhanced self-reflection with autonomous capabilities...\")\n        tui.update_status(\"Analyzing performance for self-improvement\")\n        \n        try:\n            # Gather enhanced context\n            action_history = self._analyze_action_patterns()\n            file_manifest = self._load_file_manifest()\n            knowledge_base_summary = self._load_knowledge_base_summary()\n            \n            # Prepare context for enhanced reflection\n            from config import SELF_REFLECTION_PROMPT\n            \n            reflection_prompt = SELF_REFLECTION_PROMPT.format(\n                performance_logs=performance_logs,\n                source_code=json.dumps(source_code_dict, indent=2),\n                file_manifest=json.dumps(file_manifest, indent=2),\n                knowledge_base_summary=knowledge_base_summary,\n                action_history=action_history\n            )\n            \n            tui.log_thought(\"\ud83e\udde0 Analyzing current performance patterns...\")\n            tui.log_thought(\"\ud83d\udcca Loading system architecture awareness...\")\n            tui.log_thought(\"\ud83d\udd0d Reviewing tool usage patterns...\")\n            tui.log_thought(\"\ud83d\udcda Accessing long-term knowledge base...\")\n            \n            # Get comprehensive reflection\n            tui.log_thought(\"\ud83e\udd14 Starting deep self-reflection with LLM...\")\n            tui.log_action(\"LLM\", f\"Generating self-reflection with {self.llm_client.default_model}\", \"INFO\")\n            \n            response = self.llm_client.generate([{\"role\": \"user\", \"content\": reflection_prompt}], temperature=0.8)\n            \n            tui.log_action(\"LLM\", f\"\u2705 Generated {len(response.content)} chars from {response.provider}\", \"SUCCESS\")\n            tui.log_thought(f\"\ud83d\udcad LLM Response: {response.content[:100]}...\")\n            \n            # Parse and validate the response\n            try:\n                reflection_data = json.loads(response.content)\n                \n                # Process each component\n                if \"new_tools\" in reflection_data and reflection_data[\"new_tools\"]:\n                    tui.log_thought(\"Processing autonomous tool creation requests...\")\n                    for tool_spec in reflection_data[\"new_tools\"]:\n                        self._design_and_implement_tool(tool_spec)\n                \n                if \"exploration_queries\" in reflection_data and reflection_data[\"exploration_queries\"]:\n                    tui.log_thought(\"Conducting curiosity-driven exploration...\")\n                    exploration_insights = self._conduct_exploration(reflection_data[\"exploration_queries\"])\n                    reflection_data[\"exploration_results\"] = exploration_insights\n                \n                return json.dumps(reflection_data, indent=2)\n                \n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to parse reflection response: {e}\")\n                tui.log_thought(\"Reflection response was malformed, attempting recovery...\")\n                return self._generate_fallback_improvement(source_code_dict, performance_logs)\n                \n        except Exception as e:\n            logger.error(f\"Self-reflection failed: {e}\")\n            tui.log_thought(f\"Self-reflection encountered error: {e}\")\n            return self._generate_fallback_improvement(source_code_dict, performance_logs)\n            \n            # Generate improvement suggestions\n            response = self._generate_response([\n                {\"role\": \"user\", \"content\": full_prompt}\n            ])\n            \n            # Validate and enhance the response\n            enhanced_response = self._enhance_improvement_response(response.content)\n            \n            tui.log_thought(\"Self-reflection complete - improvement plan generated\")\n            \n            return enhanced_response\n            \n        except Exception as e:\n            logger.error(f\"Error in self-reflection: {e}\")\n            tui.log_action(\"Reflection\", f\"Error during self-reflection: {e}\", \"ERROR\")\n            \n            # Return minimal valid response\n            return json.dumps({\n                \"analysis\": f\"Self-reflection failed: {e}\",\n                \"research_findings\": \"No research performed due to error\",\n                \"proposed_changes\": [],\n                \"new_tools\": []\n            })\n    \n    def _analyze_recent_performance(self) -> str:\n        \"\"\"Analyze recent task performance to identify patterns.\"\"\"\n        if not self.task_results:\n            return \"No recent performance data available\"\n        \n        recent_tasks = self.task_results[-10:]  # Last 10 tasks\n        \n        analysis = []\n        analysis.append(f\"Recent tasks analyzed: {len(recent_tasks)}\")\n        analysis.append(f\"Success rate: {sum(1 for t in recent_tasks if t.success) / len(recent_tasks):.2%}\")\n        analysis.append(f\"Average score: {sum(t.score for t in recent_tasks) / len(recent_tasks):.3f}\")\n        analysis.append(f\"Average execution time: {sum(t.execution_time for t in recent_tasks) / len(recent_tasks):.1f}s\")\n        \n        # Common errors\n        all_errors = []\n        for task in recent_tasks:\n            all_errors.extend(task.errors)\n        \n        if all_errors:\n            analysis.append(f\"Common errors: {', '.join(set(all_errors[:5]))}\")\n        \n        # Tool usage patterns\n        tool_usage = {}\n        for task in recent_tasks:\n            for tool in task.tools_used:\n                tool_usage[tool] = tool_usage.get(tool, 0) + 1\n        \n        if tool_usage:\n            most_used_tools = sorted(tool_usage.items(), key=lambda x: x[1], reverse=True)[:3]\n            analysis.append(f\"Most used tools: {', '.join([f'{tool} ({count})' for tool, count in most_used_tools])}\")\n        \n        return \"\\n\".join(analysis)\n    \n    def _identify_weaknesses(self) -> str:\n        \"\"\"Identify current weaknesses based on performance data.\"\"\"\n        weaknesses = []\n        \n        if not self.task_results:\n            return \"Insufficient data to identify weaknesses\"\n        \n        recent_tasks = self.task_results[-10:]\n        \n        # Low success rate\n        success_rate = sum(1 for t in recent_tasks if t.success) / len(recent_tasks)\n        if success_rate < 0.6:\n            weaknesses.append(f\"Low success rate ({success_rate:.1%})\")\n        \n        # Slow execution\n        avg_time = sum(t.execution_time for t in recent_tasks) / len(recent_tasks)\n        if avg_time > 180:  # 3 minutes\n            weaknesses.append(f\"Slow execution (avg: {avg_time:.1f}s)\")\n        \n        # Limited tool usage\n        unique_tools_used = set()\n        for task in recent_tasks:\n            unique_tools_used.update(task.tools_used)\n        \n        available_tools = len(self.tool_manager.tools)\n        if len(unique_tools_used) < available_tools * 0.3:\n            weaknesses.append(\"Limited tool utilization\")\n        \n        # Frequent errors\n        error_rate = sum(len(t.errors) for t in recent_tasks) / len(recent_tasks)\n        if error_rate > 1:\n            weaknesses.append(f\"High error rate ({error_rate:.1f} errors per task)\")\n        \n        return \"; \".join(weaknesses) if weaknesses else \"No major weaknesses identified\"\n    \n    def _load_knowledge_base(self):\n        \"\"\"Load the agent's long-term knowledge base.\"\"\"\n        try:\n            if os.path.exists(self.knowledge_base_path):\n                with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    \n                # Parse knowledge base sections\n                sections = content.split('## ')\n                for section in sections:\n                    if section.strip() and not section.startswith('#'):\n                        lines = section.split('\\n')\n                        section_title = lines[0].strip()\n                        section_content = '\\n'.join(lines[1:]).strip()\n                        if section_content and section_content != \"*This section will be populated*\":\n                            self.stored_knowledge[section_title] = section_content\n                            \n                logger.info(f\"Loaded {len(self.stored_knowledge)} knowledge sections\")\n        except Exception as e:\n            logger.warning(f\"Failed to load knowledge base: {e}\")\n    \n    def _track_action_pattern(self, action_type: str, details: Dict[str, Any]):\n        \"\"\"Track recurring action patterns for tool creation opportunities.\"\"\"\n        pattern_key = f\"{action_type}_{details.get('target', 'unknown')}\"\n        \n        if pattern_key not in self.action_patterns:\n            self.action_patterns[pattern_key] = []\n        \n        pattern_entry = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"action_type\": action_type,\n            \"details\": details,\n            \"tools_used\": details.get(\"tools_used\", [])\n        }\n        \n        self.action_patterns[pattern_key].append(pattern_entry)\n        \n        # Check if this pattern has become repetitive (3+ occurrences)\n        if len(self.action_patterns[pattern_key]) >= 3:\n            self._analyze_pattern_for_tool_creation(pattern_key)\n    \n    def _analyze_pattern_for_tool_creation(self, pattern_key: str):\n        \"\"\"Analyze if a repetitive pattern warrants tool creation.\"\"\"\n        pattern_instances = self.action_patterns[pattern_key]\n        \n        # Check if pattern already identified\n        existing_pattern = any(p[\"pattern_key\"] == pattern_key for p in self.repetitive_patterns)\n        if existing_pattern:\n            return\n        \n        # Analyze pattern complexity and frequency\n        tool_complexity = sum(len(instance[\"tools_used\"]) for instance in pattern_instances)\n        avg_complexity = tool_complexity / len(pattern_instances)\n        \n        # Only suggest tool creation for patterns with sufficient complexity\n        if avg_complexity >= 2:  # Multiple tools typically used\n            pattern_analysis = {\n                \"pattern_key\": pattern_key,\n                \"frequency\": len(pattern_instances),\n                \"complexity\": avg_complexity,\n                \"first_occurrence\": pattern_instances[0][\"timestamp\"],\n                \"latest_occurrence\": pattern_instances[-1][\"timestamp\"],\n                \"suggested_tool_name\": self._generate_tool_name_suggestion(pattern_key),\n                \"pattern_summary\": self._summarize_pattern(pattern_instances)\n            }\n            \n            self.repetitive_patterns.append(pattern_analysis)\n            logger.info(f\"Identified repetitive pattern for tool creation: {pattern_key}\")\n    \n    def _generate_tool_name_suggestion(self, pattern_key: str) -> str:\n        \"\"\"Generate a suggested name for a tool based on the pattern.\"\"\"\n        parts = pattern_key.split('_')\n        if len(parts) >= 2:\n            action = parts[0]\n            target = parts[1]\n            return f\"{target}_{action}_tool\"\n        return f\"{pattern_key}_tool\"\n    \n    def _summarize_pattern(self, pattern_instances: List[Dict[str, Any]]) -> str:\n        \"\"\"Create a summary of what the pattern does.\"\"\"\n        if not pattern_instances:\n            return \"Unknown pattern\"\n        \n        first_instance = pattern_instances[0]\n        action_type = first_instance[\"action_type\"]\n        tools_used = set()\n        for instance in pattern_instances:\n            tools_used.update(instance.get(\"tools_used\", []))\n        \n        return f\"Repeatedly {action_type} using tools: {', '.join(sorted(tools_used))}\"\n    \n    def _research_error_solutions(self, error_msg: str, task: Dict[str, Any]) -> str:\n        \"\"\"Research solutions for specific errors using web search.\"\"\"\n        try:\n            tui.log_action(\"web_search\", f\"Researching solutions for error: {error_msg[:50]}...\", \"TOOL\")\n            \n            # Create targeted search queries based on error and task context\n            repo_name = task.get(\"repo\", \"\").split(\"/\")[-1] if task.get(\"repo\") else \"\"\n            \n            search_queries = [\n                f\"{error_msg} {repo_name} python fix solution\",\n                f\"how to fix {error_msg[:30]} error programming\",\n                f\"{error_msg[:40]} debugging solution\"\n            ]\n            \n            research_findings = []\n            \n            for query in search_queries[:2]:  # Limit to 2 queries to avoid excessive searches\n                try:\n                    results = self.tool_manager.execute_tool(\"web_search\", query, max_results=3)\n                    \n                    for result in results[:2]:  # Top 2 results per query\n                        try:\n                            # Extract relevant text from search results\n                            if result.get(\"body\"):\n                                # Use the search snippet as it's usually most relevant\n                                summary = result[\"body\"][:200] + \"...\" if len(result[\"body\"]) > 200 else result[\"body\"]\n                                research_findings.append(f\"From {result.get('title', 'source')}: {summary}\")\n                        except Exception:\n                            continue\n                    \n                except Exception as e:\n                    logger.warning(f\"Error research query failed: {query} - {e}\")\n                    continue\n            \n            if research_findings:\n                return \"\\n\\n\".join(research_findings[:4])  # Top 4 findings\n            else:\n                return f\"No specific research found for error: {error_msg}\"\n            \n        except Exception as e:\n            logger.error(f\"Error research failed: {e}\")\n            return f\"Error research failed: {e}\"\n\n    def _research_improvements(self, weaknesses: str) -> str:\n        \"\"\"Research potential improvements using web search as designed.\"\"\"\n        try:\n            tui.log_thought(\"Researching improvements via web search...\")\n            \n            # Generate focused search queries using LLM\n            query_prompt = f\"\"\"\nBased on these identified weaknesses in a software engineering AI agent:\n\n{weaknesses}\n\nGenerate 3-4 focused search queries to find specific solutions, techniques, or best practices that could address these issues. Focus on:\n- Specific technical solutions\n- Algorithm improvements  \n- Error handling patterns\n- Performance optimization techniques\n\nReturn only the search queries, one per line.\n\"\"\"\n            \n            response = self.llm_client.generate([{\"role\": \"user\", \"content\": query_prompt}], temperature=0.3)\n            search_queries = [q.strip() for q in response.content.split('\\n') if q.strip()]\n            \n            # Perform web searches\n            research_findings = []\n            for query in search_queries[:3]:  # Limit to 3 queries\n                try:\n                    tui.log_thought(f\"Searching: {query}\")\n                    search_results = self.tool_manager.execute_tool(\"web_search\", {\"query\": query})\n                    \n                    if search_results and search_results.get(\"success\"):\n                        research_findings.append(f\"Query: {query}\")\n                        for result in search_results.get(\"results\", [])[:2]:  # Top 2 results per query\n                            research_findings.append(f\"- {result.get('title', 'Unknown')}: {result.get('snippet', 'No description')}\")\n                    \n                except Exception as e:\n                    logger.warning(f\"Web search failed for query '{query}': {e}\")\n                    research_findings.append(f\"Search failed for: {query}\")\n            \n            # If web search fails completely, fall back to internal analysis\n            if not research_findings:\n                tui.log_thought(\"Web search unavailable, using internal analysis...\")\n                research_findings = [\n                    \"Performance Analysis: Focus on improving algorithm efficiency and reducing computational complexity\",\n                    \"Code Quality: Implement better error handling and validation to prevent failures\", \n                    \"Testing Strategy: Enhance testing coverage and add edge case validation\",\n                    \"Resource Management: Optimize memory usage and processing time\",\n                    \"Error Recovery: Implement robust fallback mechanisms for failed operations\"\n                ]\n            \n            # Synthesize findings using LLM\n            synthesis_prompt = f\"\"\"\nBased on the following research findings about software engineering improvements:\n\n{chr(10).join(research_findings)}\n\nAnd the specific weaknesses identified:\n{weaknesses}\n\nSynthesize the most relevant and actionable improvement recommendations. Focus on concrete technical solutions that can be implemented in code. Provide 3-5 specific recommendations.\n\"\"\"\n            \n            synthesized_recommendations = self.llm_client.generate([{\"role\": \"user\", \"content\": synthesis_prompt}], temperature=0.4)\n            \n            return f\"Research Findings:\\n{chr(10).join(research_findings)}\\n\\nSynthesized Recommendations:\\n{synthesized_recommendations.content}\"\n            \n        except Exception as e:\n            logger.error(f\"Research improvements failed: {e}\")\n            # Fallback to basic internal analysis\n            return f\"Research failed ({e}), using basic analysis: Focus on error handling, performance optimization, and robust testing patterns.\"\n            relevant_findings = []\n            weakness_lower = weaknesses.lower()\n            \n            for finding in research_findings:\n                if any(keyword in weakness_lower for keyword in [\"performance\", \"error\", \"test\", \"memory\", \"fail\"]):\n                    relevant_findings.append(finding)\n            \n            if not relevant_findings:\n                relevant_findings = research_findings[:3]  # Default top 3\n            \n            return \"\\n\\n\".join(relevant_findings)\n            \n        except Exception as e:\n            logger.error(f\"Internal analysis failed: {e}\")\n            return \"Unable to complete improvement analysis - relying on basic optimization strategies\"\n    \n    def _enhance_improvement_response(self, response: str) -> str:\n        \"\"\"Enhance and validate the improvement response.\"\"\"\n        try:\n            # Clean the response - remove any markdown formatting\n            cleaned_response = response.strip()\n            \n            # Remove ```json and ``` markers if present\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response[7:]\n            if cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response[3:]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response[:-3]\n            \n            cleaned_response = cleaned_response.strip()\n            \n            # Try to parse as JSON\n            try:\n                data = json.loads(cleaned_response)\n            except json.JSONDecodeError:\n                # Try to extract JSON from the response\n                import re\n                json_pattern = r'\\{.*\\}'\n                matches = re.search(json_pattern, cleaned_response, re.DOTALL)\n                if matches:\n                    data = json.loads(matches.group())\n                else:\n                    raise ValueError(\"No valid JSON found in response\")\n            \n            # Ensure required fields exist\n            if \"analysis\" not in data:\n                data[\"analysis\"] = \"Generated analysis from response\"\n            \n            if \"research_findings\" not in data:\n                data[\"research_findings\"] = \"Internal analysis performed\"\n            \n            if \"proposed_changes\" not in data:\n                data[\"proposed_changes\"] = []\n            \n            if \"new_tools\" not in data:\n                data[\"new_tools\"] = []\n            \n            # Validate proposed changes structure\n            for i, change in enumerate(data[\"proposed_changes\"]):\n                if not isinstance(change, dict):\n                    data[\"proposed_changes\"][i] = {\n                        \"file_path\": \"agent/agent_core.py\",\n                        \"action\": \"replace_block\", \n                        \"identifier\": \"solve_task\",\n                        \"new_code\": \"# Improved implementation needed\",\n                        \"reasoning\": \"Change format was invalid\"\n                    }\n                    continue\n                    \n                if \"file_path\" not in change:\n                    change[\"file_path\"] = \"agent/agent_core.py\"\n                if \"action\" not in change:\n                    change[\"action\"] = \"replace_block\"\n                if \"identifier\" not in change:\n                    change[\"identifier\"] = \"solve_task\"\n                if \"new_code\" not in change:\n                    change[\"new_code\"] = \"# Improved implementation needed\"\n                if \"reasoning\" not in change:\n                    change[\"reasoning\"] = \"Improvement needed\"\n            \n            # Validate new tools structure\n            for i, tool in enumerate(data[\"new_tools\"]):\n                if not isinstance(tool, dict):\n                    data[\"new_tools\"][i] = {\n                        \"tool_name\": \"improved_tool\",\n                        \"function_name\": \"improved_function\",\n                        \"code\": \"# Tool implementation needed\",\n                        \"dependencies\": []\n                    }\n                    continue\n                    \n                if \"tool_name\" not in tool:\n                    tool[\"tool_name\"] = \"improved_tool\"\n                if \"function_name\" not in tool:\n                    tool[\"function_name\"] = \"improved_function\"\n                if \"code\" not in tool:\n                    tool[\"code\"] = \"# Tool implementation needed\"\n                if \"dependencies\" not in tool:\n                    tool[\"dependencies\"] = []\n            \n            return json.dumps(data, indent=2)\n            \n        except Exception as e:\n            logger.warning(f\"Failed to parse improvement response: {e}\")\n            logger.debug(f\"Original response: {response[:200]}...\")\n            \n            # If all parsing fails, create a minimal valid response\n            return json.dumps({\n                \"analysis\": f\"Response parsing failed: {str(e)}. Original response was: {response[:100]}...\",\n                \"research_findings\": \"Unable to parse research findings from response\",\n                \"proposed_changes\": [\n                    {\n                        \"file_path\": \"agent/agent_core.py\",\n                        \"action\": \"replace_block\",\n                        \"identifier\": \"_enhance_improvement_response\",\n                        \"new_code\": \"# Improved response parsing needed\",\n                        \"reasoning\": \"Current parsing method failed, needs better error handling\"\n                    }\n                ],\n                \"new_tools\": []\n            }, indent=2)\n    \n    def _format_available_tools(self) -> str:\n        \"\"\"Format available tools for prompt inclusion.\"\"\"\n        tools = self.tool_manager.list_tools()\n        \n        tool_descriptions = []\n        for tool in tools:\n            params = \", \".join([f\"{name}: {info.get('type', 'Any')}\" for name, info in tool[\"parameters\"].items()])\n            tool_descriptions.append(f\"- {tool['name']}({params}): {tool['description']}\")\n        \n        return \"\\n\".join(tool_descriptions)\n    \n    def _generate_response(self, messages: List[Dict[str, str]]) -> LLMResponse:\n        \"\"\"Generate a response using the LLM with system prompt.\"\"\"\n        return self.llm_client.generate(\n            messages=messages,\n            system_prompt=SYSTEM_PROMPT,\n            temperature=0.7,\n            max_tokens=4000\n        )\n    \n    def _update_performance_metrics(self):\n        \"\"\"Update overall performance metrics.\"\"\"\n        if not self.task_results:\n            return\n        \n        # Calculate total score and success rate\n        self.total_score = sum(task.score for task in self.task_results)\n        self.success_rate = sum(1 for task in self.task_results if task.success) / len(self.task_results)\n        \n        # Update TUI with latest performance\n        avg_score = self.total_score / len(self.task_results)\n        tui.add_generation(self.agent_id, self.parent_id, avg_score, self.generation)\n    \n    def get_source_code(self) -> Dict[str, str]:\n        \"\"\"Get the current source code of the agent.\"\"\"\n        source_files = [\n            \"agent/agent_core.py\",\n            \"agent/prompts.py\"\n        ]\n        \n        source_code = {}\n        for file_path in source_files:\n            try:\n                content = self.tool_manager.execute_tool(\"read_file\", file_path)\n                source_code[file_path] = content\n            except Exception as e:\n                logger.warning(f\"Could not read source file {file_path}: {e}\")\n                source_code[file_path] = f\"# Error reading file: {e}\"\n        \n        return source_code\n    \n    def get_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a summary of agent performance.\"\"\"\n        if not self.task_results:\n            return {\n                \"agent_id\": self.agent_id,\n                \"generation\": self.generation,\n                \"total_tasks\": 0,\n                \"success_rate\": 0.0,\n                \"average_score\": 0.0,\n                \"total_score\": 0.0\n            }\n        \n        return {\n            \"agent_id\": self.agent_id,\n            \"generation\": self.generation,\n            \"parent_id\": self.parent_id,\n            \"total_tasks\": len(self.task_results),\n            \"success_rate\": self.success_rate,\n            \"average_score\": self.total_score / len(self.task_results),\n            \"total_score\": self.total_score,\n            \"tools_available\": len(self.tool_manager.tools),\n            \"recent_performance\": self._analyze_recent_performance()\n        }\n    \n    def create_child_agent(self, mutation_result: Dict[str, Any]) -> 'PrometheusAgent':\n        \"\"\"Create a child agent after successful mutation.\"\"\"\n        child_id = f\"agent_{uuid.uuid4().hex[:8]}\"\n        child_agent = PrometheusAgent(child_id, self.project_root)\n        child_agent.generation = self.generation + 1\n        child_agent.parent_id = self.agent_id\n        \n        logger.info(f\"Created child agent {child_id} from parent {self.agent_id}\")\n        return child_agent\n\n# ==================== PILLAR 1: AUTONOMOUS TOOL CREATION ====================\n    \n    def _analyze_action_patterns(self) -> str:\n        \"\"\"Analyze recent actions to identify repetitive patterns suitable for tool abstraction.\"\"\"\n        try:\n            # For now, return a simple analysis - in a full implementation, this would\n            # analyze actual action logs stored during task execution\n            action_patterns = [\n                \"File reading and parsing operations (3 instances in last 5 tasks)\",\n                \"AST traversal for function extraction (4 instances)\", \n                \"JSON data transformation and validation (2 instances)\",\n                \"Error handling pattern repetition (5+ instances)\"\n            ]\n            \n            analysis = \"REPETITIVE ACTION PATTERNS DETECTED:\\n\"\n            for i, pattern in enumerate(action_patterns, 1):\n                analysis += f\"{i}. {pattern}\\n\"\n            \n            return analysis\n            \n        except Exception as e:\n            logger.error(f\"Action pattern analysis failed: {e}\")\n            return \"Unable to analyze action patterns due to error.\"\n    \n    def _design_and_implement_tool(self, tool_spec: Dict[str, Any]) -> bool:\n        \"\"\"\n        Design and implement a new tool based on specification.\n        Implements the full cognitive architecture for tool creation.\n        \"\"\"\n        try:\n            tui.log_thought(f\"Designing new tool: {tool_spec.get('tool_name', 'unnamed')}\")\n            \n            # Phase 1: Research implementation approach\n            research_queries = tool_spec.get(\"research_queries\", [])\n            research_findings = []\n            \n            for query in research_queries[:3]:  # Limit to 3 queries\n                tui.log_thought(f\"Researching: {query}\")\n                try:\n                    search_results = self.tool_manager.execute_tool(\"web_search\", {\"query\": query})\n                    if search_results and search_results.get(\"success\"):\n                        findings = f\"Query: {query}\\n\"\n                        for result in search_results.get(\"results\", [])[:2]:\n                            findings += f\"- {result.get('title', 'Unknown')}: {result.get('snippet', 'No description')}\\n\"\n                        research_findings.append(findings)\n                except Exception as e:\n                    logger.warning(f\"Research query failed: {e}\")\n            \n            # Phase 2: Generate implementation based on research\n            implementation_prompt = f\"\"\"\nBased on the following research findings, implement a Python tool function:\n\nTool Name: {tool_spec.get('tool_name')}\nPurpose: {tool_spec.get('purpose')}\nRequired Capabilities: {tool_spec.get('required_capabilities', [])}\n\nResearch Findings:\n{chr(10).join(research_findings)}\n\nGenerate a complete Python function that:\n1. Has a clear, descriptive name\n2. Includes comprehensive docstring\n3. Handles errors gracefully\n4. Returns structured data\n5. Uses best practices from the research\n\nReturn only the function implementation, no additional text.\n\"\"\"\n            \n            tui.log_thought(\"Generating tool implementation...\")\n            implementation = self.llm_client.generate([{\"role\": \"user\", \"content\": implementation_prompt}], temperature=0.4)\n            \n            # Phase 3: Create the tool\n            tool_data = {\n                \"name\": tool_spec.get(\"tool_name\"),\n                \"function_name\": tool_spec.get(\"function_name\", tool_spec.get(\"tool_name\")),\n                \"code\": implementation.content,\n                \"dependencies\": tool_spec.get(\"dependencies\", []),\n                \"description\": tool_spec.get(\"purpose\", \"Auto-generated tool\")\n            }\n            \n            success = self.tool_manager.create_new_tool(tool_data)\n            \n            if success:\n                tui.log_action(\"Tool Creation\", f\"Successfully created {tool_spec.get('tool_name')}\", \"SUCCESS\")\n                return True\n            else:\n                tui.log_action(\"Tool Creation\", f\"Failed to create {tool_spec.get('tool_name')}\", \"ERROR\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Tool design and implementation failed: {e}\")\n            tui.log_action(\"Tool Creation\", f\"Tool creation error: {e}\", \"ERROR\")\n            return False\n    \n    # ==================== PILLAR 2: SELF-MODIFICATION ====================\n    \n    def _load_file_manifest(self) -> Dict[str, Any]:\n        \"\"\"Load the system architecture manifest for self-modification awareness.\"\"\"\n        try:\n            manifest_path = os.path.join(self.project_root, \"file_manifest.json\")\n            if os.path.exists(manifest_path):\n                with open(manifest_path, 'r', encoding='utf-8') as f:\n                    return json.load(f)\n            else:\n                logger.warning(\"File manifest not found, using minimal default\")\n                return {\"system_architecture\": {}, \"evolutionary_components\": {}}\n        except Exception as e:\n            logger.error(f\"Failed to load file manifest: {e}\")\n            return {\"error\": \"Unable to load system architecture information\"}\n    \n    def _assess_architectural_stagnation(self) -> Dict[str, Any]:\n        \"\"\"Assess whether performance stagnation is due to architectural limitations.\"\"\"\n        try:\n            # Analyze recent performance trends\n            recent_scores = [result.score for result in self.task_results[-10:]]\n            \n            if len(recent_scores) < 5:\n                return {\"assessment\": \"insufficient_data\", \"recommendation\": \"continue_current_approach\"}\n            \n            # Check for stagnation patterns\n            score_variance = max(recent_scores) - min(recent_scores)\n            average_score = sum(recent_scores) / len(recent_scores)\n            \n            if score_variance < 0.1 and average_score < 0.6:\n                return {\n                    \"assessment\": \"architectural_stagnation\",\n                    \"evidence\": f\"Low variance ({score_variance:.3f}) and low average ({average_score:.3f})\",\n                    \"recommendation\": \"consider_algorithmic_changes\",\n                    \"confidence\": 0.8\n                }\n            else:\n                return {\n                    \"assessment\": \"normal_variation\", \n                    \"recommendation\": \"continue_current_approach\",\n                    \"confidence\": 0.6\n                }\n                \n        except Exception as e:\n            logger.error(f\"Architectural assessment failed: {e}\")\n            return {\"assessment\": \"error\", \"recommendation\": \"unable_to_assess\"}\n    \n    # ==================== PILLAR 3: CURIOSITY-DRIVEN EXPLORATION ====================\n    \n    def _load_knowledge_base_summary(self) -> str:\n        \"\"\"Load a summary of the long-term knowledge base.\"\"\"\n        try:\n            kb_path = os.path.join(self.project_root, \"archive\", \"knowledge_base.md\")\n            if os.path.exists(kb_path):\n                with open(kb_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                # Extract key sections for summary\n                lines = content.split('\\n')\n                summary_lines = []\n                in_summary_section = False\n                \n                for line in lines:\n                    if '## Key Insights' in line or '## Research Discoveries' in line:\n                        in_summary_section = True\n                    elif '##' in line and in_summary_section:\n                        break\n                    elif in_summary_section:\n                        summary_lines.append(line)\n                \n                if summary_lines:\n                    return '\\n'.join(summary_lines[:20])  # First 20 lines of insights\n                else:\n                    return \"Knowledge base exists but no key insights section found.\"\n            else:\n                return \"No knowledge base found - this is generation 0 or the KB was not created.\"\n                \n        except Exception as e:\n            logger.error(f\"Failed to load knowledge base summary: {e}\")\n            return \"Error loading knowledge base summary.\"\n    \n    def _conduct_exploration(self, exploration_queries: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n        \"\"\"Conduct curiosity-driven exploration based on generated queries.\"\"\"\n        exploration_results = []\n        \n        for query_spec in exploration_queries[:3]:  # Limit exploration to avoid excessive time\n            try:\n                tui.log_thought(f\"Exploring: {query_spec.get('question', 'Unknown question')}\")\n                \n                # Generate specific search queries\n                search_query = query_spec.get(\"research_focus\", query_spec.get(\"question\", \"\"))\n                \n                # Conduct web search\n                search_results = self.tool_manager.execute_tool(\"web_search\", {\"query\": search_query})\n                \n                if search_results and search_results.get(\"success\"):\n                    # Synthesize findings\n                    synthesis_prompt = f\"\"\"\nBased on the following web search results for the research question: \"{query_spec.get('question')}\"\n\nSearch Results:\n{json.dumps(search_results.get('results', [])[:3], indent=2)}\n\nProvide a concise synthesis of the key insights that could be relevant to an AI agent working on software engineering tasks. Focus on:\n1. Novel techniques or approaches\n2. Emerging best practices\n3. Fundamental insights that could change how problems are approached\n\nFormat as a brief summary (2-3 paragraphs max).\n\"\"\"\n                    \n                    synthesis = self.llm_client.generate([{\"role\": \"user\", \"content\": synthesis_prompt}], temperature=0.6)\n                    \n                    exploration_results.append({\n                        \"question\": query_spec.get(\"question\"),\n                        \"research_focus\": query_spec.get(\"research_focus\"),\n                        \"synthesis\": synthesis.content,\n                        \"potential_impact\": query_spec.get(\"potential_impact\"),\n                        \"timestamp\": datetime.now().isoformat()\n                    })\n                    \n                    tui.log_thought(f\"Exploration completed: {len(synthesis.content.split())} words of insights\")\n                else:\n                    exploration_results.append({\n                        \"question\": query_spec.get(\"question\"),\n                        \"synthesis\": \"Web search failed or returned no useful results\",\n                        \"potential_impact\": \"Unknown due to search failure\",\n                        \"timestamp\": datetime.now().isoformat()\n                    })\n                    \n            except Exception as e:\n                logger.error(f\"Exploration query failed: {e}\")\n                exploration_results.append({\n                    \"question\": query_spec.get(\"question\", \"Unknown\"),\n                    \"synthesis\": f\"Exploration failed due to error: {e}\",\n                    \"potential_impact\": \"Unknown due to error\",\n                    \"timestamp\": datetime.now().isoformat()\n                })\n        \n        # Store insights in knowledge base\n        self._update_knowledge_base(exploration_results)\n        \n        return exploration_results\n    \n    def _update_knowledge_base(self, exploration_results: List[Dict[str, Any]]) -> None:\n        \"\"\"Update the long-term knowledge base with new exploration insights.\"\"\"\n        try:\n            kb_path = os.path.join(self.project_root, \"archive\", \"knowledge_base.md\")\n            \n            # Prepare new content\n            new_entries = []\n            for result in exploration_results:\n                if \"synthesis\" in result and len(result[\"synthesis\"]) > 50:  # Only meaningful results\n                    entry = f\"\"\"\n### {result.get('question', 'Unknown Question')}\n*Explored: {result.get('timestamp', 'Unknown time')}*\n**Research Focus**: {result.get('research_focus', 'General inquiry')}\n**Insights**: {result.get('synthesis', 'No insights available')}\n**Potential Impact**: {result.get('potential_impact', 'Unknown')}\n\"\"\"\n                    new_entries.append(entry)\n            \n            if new_entries:\n                # Append to knowledge base\n                with open(kb_path, 'a', encoding='utf-8') as f:\n                    f.write(f\"\\n\\n## Exploration Results - {datetime.now().strftime('%Y-%m-%d')}\\n\")\n                    f.write('\\n'.join(new_entries))\n                \n                tui.log_action(\"Knowledge Base\", f\"Added {len(new_entries)} new insights\", \"SUCCESS\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to update knowledge base: {e}\")\n    \n    def _generate_fallback_improvement(self, source_code_dict: Dict[str, str], performance_logs: str) -> str:\n        \"\"\"Generate a basic improvement proposal when enhanced reflection fails.\"\"\"\n        return json.dumps({\n            \"cognitive_analysis\": {\n                \"repetitive_patterns\": \"Unable to analyze due to reflection failure\",\n                \"architectural_assessment\": \"Fallback mode - basic improvement only\",\n                \"knowledge_gaps\": \"Exploration capabilities unavailable\"\n            },\n            \"proposed_changes\": [{\n                \"file_path\": \"agent/agent_core.py\",\n                \"action\": \"modify_function\",\n                \"identifier\": \"_analyze_recent_performance\",\n                \"new_code\": \"# Fallback improvement - enhanced error handling\\nreturn 'Performance analysis in fallback mode'\",\n                \"reasoning\": \"Fallback improvement to maintain functionality\",\n                \"confidence\": 0.3,\n                \"impact_level\": \"LOW\"\n            }],\n            \"new_tools\": [],\n            \"exploration_queries\": [],\n            \"synthesis\": \"System in fallback mode due to reflection processing error\"\n        })\n\nif __name__ == \"__main__\":\n    # Test the agent\n    import tempfile\n    \n    with tempfile.TemporaryDirectory() as temp_dir:\n        # Create test agent\n        agent = PrometheusAgent(project_root=temp_dir)\n        \n        # Test task solving\n        test_task = {\n            \"instance_id\": \"test_task_1\",\n            \"problem_statement\": \"Write a function to calculate factorial\",\n            \"repo\": \"test_repo\",\n            \"base_commit\": \"abc123\"\n        }\n        \n        print(\"Testing task solving...\")\n        result = agent.solve_task(test_task)\n        print(f\"Task result: {result}\")\n        \n        # Test self-reflection\n        print(\"\\nTesting self-reflection...\")\n        source_code = agent.get_source_code()\n        performance_logs = \"Recent performance has been suboptimal\"\n        \n        improvement_json = agent.self_reflect_and_improve(source_code, performance_logs)\n        print(f\"Improvement suggestions: {improvement_json}\")\n        \n        # Show performance summary\n        print(\"\\nPerformance summary:\")\n        summary = agent.get_performance_summary()\n        for key, value in summary.items():\n            print(f\"  {key}: {value}\")\n",
    "agent/prompts.py": "\"\"\"Agent prompts and templates for Prometheus 2.0.\"\"\"\n\nfrom config import AGENT_SYSTEM_PROMPT, SELF_REFLECTION_PROMPT\n\n# Main system prompt for the agent\nSYSTEM_PROMPT = AGENT_SYSTEM_PROMPT\n\n# Self-reflection prompt template\nREFLECTION_PROMPT = SELF_REFLECTION_PROMPT\n\n# Problem-solving prompt template\nPROBLEM_SOLVING_PROMPT = \"\"\"You are tasked with solving the following software engineering problem:\n\nPROBLEM DESCRIPTION:\n{problem_statement}\n\nREPOSITORY: {repo_name}\nBASE COMMIT: {base_commit}\n\nAVAILABLE TOOLS:\n{available_tools}\n\nYour approach should be:\n1. Understand the problem thoroughly\n2. Use web_search to find relevant information if needed\n3. Examine the codebase using read_file and list_directory\n4. Develop a solution strategy\n5. Implement the solution by writing/modifying files\n6. Test your solution if possible\n\nThink step by step and explain your reasoning. Always use the available tools to gather information and implement your solution.\n\nRemember: You must generate actual code changes that solve the described problem.\"\"\"\n\n# Code analysis prompt\nCODE_ANALYSIS_PROMPT = \"\"\"Analyze the following code and identify potential improvements:\n\nCODE:\n{code}\n\nCONTEXT:\n- File: {file_path}\n- Purpose: {purpose}\n- Current issues: {issues}\n\nPlease provide:\n1. Code quality assessment\n2. Potential bugs or issues\n3. Performance improvements\n4. Best practice recommendations\n5. Specific code changes needed\n\nFocus on practical, implementable improvements.\"\"\"\n\n# Tool creation prompt\nTOOL_CREATION_PROMPT = \"\"\"You need to create a new tool to solve a specific problem.\n\nPROBLEM: {problem_description}\nEXISTING TOOLS: {existing_tools}\n\nDesign and implement a new tool that addresses this problem. The tool should:\n1. Have a clear, descriptive name\n2. Include proper documentation\n3. Handle errors gracefully\n4. Follow Python best practices\n5. Be reusable for similar problems\n\nProvide the complete tool implementation as Python code.\"\"\"\n\n# Performance improvement prompt\nIMPROVEMENT_PROMPT = \"\"\"Based on your recent performance data, identify specific areas for improvement:\n\nPERFORMANCE DATA:\n{performance_data}\n\nCURRENT APPROACH:\n{current_approach}\n\nKNOWN ISSUES:\n{known_issues}\n\nResearch and propose specific improvements to:\n1. Problem-solving methodology\n2. Code implementation strategies\n3. Tool usage efficiency\n4. Error handling and recovery\n\nUse web search to find better approaches if needed. Provide concrete, implementable changes.\"\"\"\n"
  },
  "metadata": {
    "shutdown_iteration": 1,
    "final_agent": true
  }
}