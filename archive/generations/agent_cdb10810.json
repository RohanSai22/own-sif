{
  "agent_id": "agent_cdb10810",
  "generation": 0,
  "parent_id": null,
  "created_at": "2025-06-25T22:06:51.197220",
  "performance_score": 0.7999999999999999,
  "success_rate": 1.0,
  "total_tasks": 1,
  "task_results": [],
  "mutations_applied": [],
  "source_code": {
    "agent/agent_core.py": "\"\"\"Core agent implementation for Prometheus 2.0 - The self-improving AI agent.\"\"\"\n\nimport json\nimport os\nimport time\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nimport logging\n\nfrom llm_provider.unified_client import llm_client, LLMResponse\nfrom tools.tool_manager import ToolManager\nfrom agent.prompts import (\n    SYSTEM_PROMPT, REFLECTION_PROMPT, PROBLEM_SOLVING_PROMPT,\n    CODE_ANALYSIS_PROMPT, IMPROVEMENT_PROMPT\n)\nfrom framework.tui import tui\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of solving a task.\"\"\"\n    task_id: str\n    success: bool\n    score: float\n    solution: str\n    execution_time: float\n    errors: List[str]\n    tools_used: List[str]\n    reasoning: str\n\nclass PrometheusAgent:\n    \"\"\"The core self-improving AI agent.\"\"\"\n    \n    def __init__(self, agent_id: str = None, project_root: str = \".\"):\n        self.agent_id = agent_id or f\"agent_{uuid.uuid4().hex[:8]}\"\n        self.project_root = project_root\n        self.generation = 0\n        self.parent_id = None\n        \n        # Initialize components\n        self.tool_manager = ToolManager(project_root)\n        self.llm_client = llm_client\n        \n        # Performance tracking\n        self.task_results: List[TaskResult] = []\n        self.total_score = 0.0\n        self.success_rate = 0.0\n        \n        # Conversation history for context\n        self.conversation_history: List[Dict[str, str]] = []\n        \n        logger.info(f\"Initialized agent {self.agent_id}\")\n    \n    def solve_task(self, task: Dict[str, Any]) -> TaskResult:\n        \"\"\"\n        Solve a single SWE-bench task.\n        \n        Args:\n            task: Task dictionary with problem description and metadata\n            \n        Returns:\n            TaskResult with the solution and performance metrics\n        \"\"\"\n        start_time = time.time()\n        task_id = task.get(\"instance_id\", f\"task_{uuid.uuid4().hex[:8]}\")\n        \n        tui.log_thought(f\"Starting task: {task_id}\")\n        tui.update_task(f\"Solving task {task_id}\")\n        \n        try:\n            # Extract task information\n            problem_statement = task.get(\"problem_statement\", \"\")\n            repo_name = task.get(\"repo\", \"\")\n            base_commit = task.get(\"base_commit\", \"\")\n            \n            # Get available tools\n            available_tools = self._format_available_tools()\n            \n            # Create the problem-solving prompt\n            prompt = PROBLEM_SOLVING_PROMPT.format(\n                problem_statement=problem_statement,\n                repo_name=repo_name,\n                base_commit=base_commit,\n                available_tools=available_tools\n            )\n            \n            tui.log_thought(\"Analyzing problem and planning approach...\")\n            \n            # Generate solution using LLM\n            solution_response = self._generate_response([\n                {\"role\": \"user\", \"content\": prompt}\n            ])\n            \n            # Execute the solution\n            execution_result = self._execute_solution(solution_response.content, task)\n            \n            execution_time = time.time() - start_time\n            \n            # Calculate score based on success and execution quality\n            score = self._calculate_task_score(execution_result, execution_time)\n            \n            # Create task result\n            result = TaskResult(\n                task_id=task_id,\n                success=execution_result[\"success\"],\n                score=score,\n                solution=solution_response.content,\n                execution_time=execution_time,\n                errors=execution_result.get(\"errors\", []),\n                tools_used=execution_result.get(\"tools_used\", []),\n                reasoning=execution_result.get(\"reasoning\", \"\")\n            )\n            \n            # Update performance tracking\n            self.task_results.append(result)\n            self._update_performance_metrics()\n            \n            # Log result\n            if result.success:\n                tui.log_action(\"Task\", f\"\u2713 Completed {task_id} (score: {score:.3f})\", \"SUCCESS\")\n            else:\n                tui.log_action(\"Task\", f\"\u2717 Failed {task_id} (score: {score:.3f})\", \"ERROR\")\n            \n            return result\n            \n        except Exception as e:\n            execution_time = time.time() - start_time\n            error_msg = str(e)\n            \n            logger.error(f\"Error solving task {task_id}: {error_msg}\")\n            tui.log_action(\"Task\", f\"\u2717 Error in {task_id}: {error_msg}\", \"ERROR\")\n            \n            # Use web search to research error-specific solutions\n            error_research = self._research_error_solutions(error_msg, task)\n            \n            # Try to recover with the research information\n            if error_research:\n                tui.log_thought(\"Attempting error recovery with research insights...\")\n                try:\n                    # Create recovery prompt with error research\n                    recovery_prompt = f\"\"\"\nORIGINAL TASK:\n{task.get(\"problem_statement\", \"\")}\n\nENCOUNTERED ERROR:\n{error_msg}\n\nRESEARCH FINDINGS:\n{error_research}\n\nBased on this error and research, please provide a corrected approach to solve the original task. Focus specifically on avoiding the error that occurred.\n\"\"\"\n                    \n                    recovery_response = self._generate_response([\n                        {\"role\": \"user\", \"content\": recovery_prompt}\n                    ])\n                    \n                    # Execute recovery solution\n                    recovery_result = self._execute_solution(recovery_response.content, task)\n                    \n                    if recovery_result[\"success\"]:\n                        execution_time = time.time() - start_time\n                        score = self._calculate_task_score(recovery_result, execution_time)\n                        \n                        result = TaskResult(\n                            task_id=task_id,\n                            success=True,\n                            score=score,\n                            solution=recovery_response.content,\n                            execution_time=execution_time,\n                            errors=[error_msg],  # Keep original error for learning\n                            tools_used=recovery_result.get(\"tools_used\", []) + [\"web_search\"],\n                            reasoning=f\"Recovered from error using web research: {recovery_result.get('reasoning', '')}\"\n                        )\n                        \n                        self.task_results.append(result)\n                        self._update_performance_metrics()\n                        \n                        tui.log_action(\"Task\", f\"\u2713 Recovered {task_id} (score: {score:.3f})\", \"SUCCESS\")\n                        return result\n                        \n                except Exception as recovery_error:\n                    logger.warning(f\"Recovery attempt failed: {recovery_error}\")\n            \n            # Return failed result\n            result = TaskResult(\n                task_id=task_id,\n                success=False,\n                score=0.0,\n                solution=\"\",\n                execution_time=execution_time,\n                errors=[error_msg],\n                tools_used=[],\n                reasoning=f\"Task failed with error: {error_msg}\"\n            )\n            \n            self.task_results.append(result)\n            self._update_performance_metrics()\n            \n            return result\n    \n    def _execute_solution(self, solution: str, task: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute the generated solution.\"\"\"\n        execution_result = {\n            \"success\": False,\n            \"errors\": [],\n            \"tools_used\": [],\n            \"reasoning\": \"\",\n            \"files_modified\": []\n        }\n        \n        try:\n            # Parse the solution to extract tool calls and reasoning\n            reasoning_parts = []\n            tools_used = []\n            \n            # This is a simplified execution - in practice, you might want\n            # to parse and execute specific tool calls from the solution\n            \n            # For now, we'll simulate execution based on solution content\n            if \"web_search\" in solution.lower():\n                tui.log_action(\"web_search\", \"Searching for relevant information\", \"TOOL\")\n                tools_used.append(\"web_search\")\n                \n            if \"read_file\" in solution.lower():\n                tui.log_action(\"read_file\", \"Reading repository files\", \"TOOL\")\n                tools_used.append(\"read_file\")\n                \n            if \"write_file\" in solution.lower():\n                tui.log_action(\"write_file\", \"Writing solution files\", \"TOOL\")\n                tools_used.append(\"write_file\")\n                \n            # Basic success criteria\n            has_code_changes = any(keyword in solution.lower() for keyword in \n                                 [\"write_file\", \"modify\", \"patch\", \"fix\", \"implement\"])\n            has_reasoning = len(solution) > 100\n            \n            execution_result.update({\n                \"success\": has_code_changes and has_reasoning,\n                \"tools_used\": tools_used,\n                \"reasoning\": solution[:500] + \"...\" if len(solution) > 500 else solution\n            })\n            \n        except Exception as e:\n            execution_result[\"errors\"].append(str(e))\n        \n        return execution_result\n    \n    def _calculate_task_score(self, execution_result: Dict[str, Any], execution_time: float) -> float:\n        \"\"\"Calculate a score for the task based on execution results.\"\"\"\n        score = 0.0\n        \n        # Base score for success\n        if execution_result[\"success\"]:\n            score += 0.5\n        \n        # Bonus for using tools effectively\n        tools_used = execution_result.get(\"tools_used\", [])\n        if tools_used:\n            score += min(0.2, len(tools_used) * 0.05)\n        \n        # Bonus for detailed reasoning\n        reasoning = execution_result.get(\"reasoning\", \"\")\n        if len(reasoning) > 200:\n            score += 0.1\n        \n        # Penalty for errors\n        errors = execution_result.get(\"errors\", [])\n        score -= len(errors) * 0.1\n        \n        # Time efficiency bonus/penalty\n        if execution_time < 60:  # Under 1 minute\n            score += 0.1\n        elif execution_time > 300:  # Over 5 minutes\n            score -= 0.1\n        \n        # Ensure score is between 0 and 1\n        return max(0.0, min(1.0, score))\n    \n    def self_reflect_and_improve(self, source_code_dict: Dict[str, str], performance_logs: str) -> str:\n        \"\"\"\n        Reflect on performance and generate self-improvement patches.\n        \n        Args:\n            source_code_dict: Dictionary of agent source code files\n            performance_logs: String containing performance analysis\n            \n        Returns:\n            JSON string with proposed improvements\n        \"\"\"\n        tui.log_thought(\"Beginning self-reflection and improvement analysis...\")\n        tui.update_status(\"Analyzing performance for self-improvement\")\n        \n        try:\n            # Prepare context for reflection\n            recent_performance = self._analyze_recent_performance()\n            current_weaknesses = self._identify_weaknesses()\n            \n            # Research potential improvements\n            tui.log_thought(\"Researching improvement strategies...\")\n            research_results = self._research_improvements(current_weaknesses)\n            \n            # Generate reflection prompt\n            reflection_prompt = REFLECTION_PROMPT.format(\n                performance_logs=performance_logs,\n                source_code=json.dumps(source_code_dict, indent=2)\n            )\n            \n            # Add context about recent performance\n            context_info = f\"\"\"\nRECENT PERFORMANCE ANALYSIS:\n{recent_performance}\n\nIDENTIFIED WEAKNESSES:\n{current_weaknesses}\n\nRESEARCH FINDINGS:\n{research_results}\n\nBased on this analysis, propose specific code improvements:\n\"\"\"\n            \n            full_prompt = context_info + reflection_prompt\n            \n            tui.log_thought(\"Generating improvement proposals...\")\n            \n            # Generate improvement suggestions\n            response = self._generate_response([\n                {\"role\": \"user\", \"content\": full_prompt}\n            ])\n            \n            # Validate and enhance the response\n            enhanced_response = self._enhance_improvement_response(response.content)\n            \n            tui.log_thought(\"Self-reflection complete - improvement plan generated\")\n            \n            return enhanced_response\n            \n        except Exception as e:\n            logger.error(f\"Error in self-reflection: {e}\")\n            tui.log_action(\"Reflection\", f\"Error during self-reflection: {e}\", \"ERROR\")\n            \n            # Return minimal valid response\n            return json.dumps({\n                \"analysis\": f\"Self-reflection failed: {e}\",\n                \"research_findings\": \"No research performed due to error\",\n                \"proposed_changes\": [],\n                \"new_tools\": []\n            })\n    \n    def _analyze_recent_performance(self) -> str:\n        \"\"\"Analyze recent task performance to identify patterns.\"\"\"\n        if not self.task_results:\n            return \"No recent performance data available\"\n        \n        recent_tasks = self.task_results[-10:]  # Last 10 tasks\n        \n        analysis = []\n        analysis.append(f\"Recent tasks analyzed: {len(recent_tasks)}\")\n        analysis.append(f\"Success rate: {sum(1 for t in recent_tasks if t.success) / len(recent_tasks):.2%}\")\n        analysis.append(f\"Average score: {sum(t.score for t in recent_tasks) / len(recent_tasks):.3f}\")\n        analysis.append(f\"Average execution time: {sum(t.execution_time for t in recent_tasks) / len(recent_tasks):.1f}s\")\n        \n        # Common errors\n        all_errors = []\n        for task in recent_tasks:\n            all_errors.extend(task.errors)\n        \n        if all_errors:\n            analysis.append(f\"Common errors: {', '.join(set(all_errors[:5]))}\")\n        \n        # Tool usage patterns\n        tool_usage = {}\n        for task in recent_tasks:\n            for tool in task.tools_used:\n                tool_usage[tool] = tool_usage.get(tool, 0) + 1\n        \n        if tool_usage:\n            most_used_tools = sorted(tool_usage.items(), key=lambda x: x[1], reverse=True)[:3]\n            analysis.append(f\"Most used tools: {', '.join([f'{tool} ({count})' for tool, count in most_used_tools])}\")\n        \n        return \"\\n\".join(analysis)\n    \n    def _identify_weaknesses(self) -> str:\n        \"\"\"Identify current weaknesses based on performance data.\"\"\"\n        weaknesses = []\n        \n        if not self.task_results:\n            return \"Insufficient data to identify weaknesses\"\n        \n        recent_tasks = self.task_results[-10:]\n        \n        # Low success rate\n        success_rate = sum(1 for t in recent_tasks if t.success) / len(recent_tasks)\n        if success_rate < 0.6:\n            weaknesses.append(f\"Low success rate ({success_rate:.1%})\")\n        \n        # Slow execution\n        avg_time = sum(t.execution_time for t in recent_tasks) / len(recent_tasks)\n        if avg_time > 180:  # 3 minutes\n            weaknesses.append(f\"Slow execution (avg: {avg_time:.1f}s)\")\n        \n        # Limited tool usage\n        unique_tools_used = set()\n        for task in recent_tasks:\n            unique_tools_used.update(task.tools_used)\n        \n        available_tools = len(self.tool_manager.tools)\n        if len(unique_tools_used) < available_tools * 0.3:\n            weaknesses.append(\"Limited tool utilization\")\n        \n        # Frequent errors\n        error_rate = sum(len(t.errors) for t in recent_tasks) / len(recent_tasks)\n        if error_rate > 1:\n            weaknesses.append(f\"High error rate ({error_rate:.1f} errors per task)\")\n        \n        return \"; \".join(weaknesses) if weaknesses else \"No major weaknesses identified\"\n    \n    def _research_error_solutions(self, error_msg: str, task: Dict[str, Any]) -> str:\n        \"\"\"Research solutions for specific errors using web search.\"\"\"\n        try:\n            tui.log_action(\"web_search\", f\"Researching solutions for error: {error_msg[:50]}...\", \"TOOL\")\n            \n            # Create targeted search queries based on error and task context\n            repo_name = task.get(\"repo\", \"\").split(\"/\")[-1] if task.get(\"repo\") else \"\"\n            \n            search_queries = [\n                f\"{error_msg} {repo_name} python fix solution\",\n                f\"how to fix {error_msg[:30]} error programming\",\n                f\"{error_msg[:40]} debugging solution\"\n            ]\n            \n            research_findings = []\n            \n            for query in search_queries[:2]:  # Limit to 2 queries to avoid excessive searches\n                try:\n                    results = self.tool_manager.execute_tool(\"web_search\", query, max_results=3)\n                    \n                    for result in results[:2]:  # Top 2 results per query\n                        try:\n                            # Extract relevant text from search results\n                            if result.get(\"body\"):\n                                # Use the search snippet as it's usually most relevant\n                                summary = result[\"body\"][:200] + \"...\" if len(result[\"body\"]) > 200 else result[\"body\"]\n                                research_findings.append(f\"From {result.get('title', 'source')}: {summary}\")\n                        except Exception:\n                            continue\n                    \n                except Exception as e:\n                    logger.warning(f\"Error research query failed: {query} - {e}\")\n                    continue\n            \n            if research_findings:\n                return \"\\n\\n\".join(research_findings[:4])  # Top 4 findings\n            else:\n                return f\"No specific research found for error: {error_msg}\"\n            \n        except Exception as e:\n            logger.error(f\"Error research failed: {e}\")\n            return f\"Error research failed: {e}\"\n\n    def _research_improvements(self, weaknesses: str) -> str:\n        \"\"\"Research potential improvements based on internal analysis only.\"\"\"\n        try:\n            # Instead of web search, use internal analysis and knowledge\n            tui.log_thought(\"Analyzing weaknesses using internal knowledge...\")\n            \n            # Generate improvement suggestions based on known patterns\n            research_findings = [\n                \"Performance Analysis: Focus on improving algorithm efficiency and reducing computational complexity\",\n                \"Code Quality: Implement better error handling and validation to prevent failures\",\n                \"Testing Strategy: Enhance testing coverage and add edge case validation\",\n                \"Resource Management: Optimize memory usage and processing time\",\n                \"Error Recovery: Implement robust fallback mechanisms for failed operations\"\n            ]\n            \n            # Filter findings based on weaknesses\n            relevant_findings = []\n            weakness_lower = weaknesses.lower()\n            \n            for finding in research_findings:\n                if any(keyword in weakness_lower for keyword in [\"performance\", \"error\", \"test\", \"memory\", \"fail\"]):\n                    relevant_findings.append(finding)\n            \n            if not relevant_findings:\n                relevant_findings = research_findings[:3]  # Default top 3\n            \n            return \"\\n\\n\".join(relevant_findings)\n            \n        except Exception as e:\n            logger.error(f\"Internal analysis failed: {e}\")\n            return \"Unable to complete improvement analysis - relying on basic optimization strategies\"\n    \n    def _enhance_improvement_response(self, response: str) -> str:\n        \"\"\"Enhance and validate the improvement response.\"\"\"\n        try:\n            # Clean the response - remove any markdown formatting\n            cleaned_response = response.strip()\n            \n            # Remove ```json and ``` markers if present\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response[7:]\n            if cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response[3:]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response[:-3]\n            \n            cleaned_response = cleaned_response.strip()\n            \n            # Try to parse as JSON\n            try:\n                data = json.loads(cleaned_response)\n            except json.JSONDecodeError:\n                # Try to extract JSON from the response\n                import re\n                json_pattern = r'\\{.*\\}'\n                matches = re.search(json_pattern, cleaned_response, re.DOTALL)\n                if matches:\n                    data = json.loads(matches.group())\n                else:\n                    raise ValueError(\"No valid JSON found in response\")\n            \n            # Ensure required fields exist\n            if \"analysis\" not in data:\n                data[\"analysis\"] = \"Generated analysis from response\"\n            \n            if \"research_findings\" not in data:\n                data[\"research_findings\"] = \"Internal analysis performed\"\n            \n            if \"proposed_changes\" not in data:\n                data[\"proposed_changes\"] = []\n            \n            if \"new_tools\" not in data:\n                data[\"new_tools\"] = []\n            \n            # Validate proposed changes structure\n            for i, change in enumerate(data[\"proposed_changes\"]):\n                if not isinstance(change, dict):\n                    data[\"proposed_changes\"][i] = {\n                        \"file_path\": \"agent/agent_core.py\",\n                        \"action\": \"replace_block\", \n                        \"identifier\": \"solve_task\",\n                        \"new_code\": \"# Improved implementation needed\",\n                        \"reasoning\": \"Change format was invalid\"\n                    }\n                    continue\n                    \n                if \"file_path\" not in change:\n                    change[\"file_path\"] = \"agent/agent_core.py\"\n                if \"action\" not in change:\n                    change[\"action\"] = \"replace_block\"\n                if \"identifier\" not in change:\n                    change[\"identifier\"] = \"solve_task\"\n                if \"new_code\" not in change:\n                    change[\"new_code\"] = \"# Improved implementation needed\"\n                if \"reasoning\" not in change:\n                    change[\"reasoning\"] = \"Improvement needed\"\n            \n            # Validate new tools structure\n            for i, tool in enumerate(data[\"new_tools\"]):\n                if not isinstance(tool, dict):\n                    data[\"new_tools\"][i] = {\n                        \"tool_name\": \"improved_tool\",\n                        \"function_name\": \"improved_function\",\n                        \"code\": \"# Tool implementation needed\",\n                        \"dependencies\": []\n                    }\n                    continue\n                    \n                if \"tool_name\" not in tool:\n                    tool[\"tool_name\"] = \"improved_tool\"\n                if \"function_name\" not in tool:\n                    tool[\"function_name\"] = \"improved_function\"\n                if \"code\" not in tool:\n                    tool[\"code\"] = \"# Tool implementation needed\"\n                if \"dependencies\" not in tool:\n                    tool[\"dependencies\"] = []\n            \n            return json.dumps(data, indent=2)\n            \n        except Exception as e:\n            logger.warning(f\"Failed to parse improvement response: {e}\")\n            logger.debug(f\"Original response: {response[:200]}...\")\n            \n            # If all parsing fails, create a minimal valid response\n            return json.dumps({\n                \"analysis\": f\"Response parsing failed: {str(e)}. Original response was: {response[:100]}...\",\n                \"research_findings\": \"Unable to parse research findings from response\",\n                \"proposed_changes\": [\n                    {\n                        \"file_path\": \"agent/agent_core.py\",\n                        \"action\": \"replace_block\",\n                        \"identifier\": \"_enhance_improvement_response\",\n                        \"new_code\": \"# Improved response parsing needed\",\n                        \"reasoning\": \"Current parsing method failed, needs better error handling\"\n                    }\n                ],\n                \"new_tools\": []\n            }, indent=2)\n    \n    def _format_available_tools(self) -> str:\n        \"\"\"Format available tools for prompt inclusion.\"\"\"\n        tools = self.tool_manager.list_tools()\n        \n        tool_descriptions = []\n        for tool in tools:\n            params = \", \".join([f\"{name}: {info.get('type', 'Any')}\" for name, info in tool[\"parameters\"].items()])\n            tool_descriptions.append(f\"- {tool['name']}({params}): {tool['description']}\")\n        \n        return \"\\n\".join(tool_descriptions)\n    \n    def _generate_response(self, messages: List[Dict[str, str]]) -> LLMResponse:\n        \"\"\"Generate a response using the LLM with system prompt.\"\"\"\n        return self.llm_client.generate(\n            messages=messages,\n            system_prompt=SYSTEM_PROMPT,\n            temperature=0.7,\n            max_tokens=4000\n        )\n    \n    def _update_performance_metrics(self):\n        \"\"\"Update overall performance metrics.\"\"\"\n        if not self.task_results:\n            return\n        \n        # Calculate total score and success rate\n        self.total_score = sum(task.score for task in self.task_results)\n        self.success_rate = sum(1 for task in self.task_results if task.success) / len(self.task_results)\n        \n        # Update TUI with latest performance\n        avg_score = self.total_score / len(self.task_results)\n        tui.add_generation(self.agent_id, self.parent_id, avg_score, self.generation)\n    \n    def get_source_code(self) -> Dict[str, str]:\n        \"\"\"Get the current source code of the agent.\"\"\"\n        source_files = [\n            \"agent/agent_core.py\",\n            \"agent/prompts.py\"\n        ]\n        \n        source_code = {}\n        for file_path in source_files:\n            try:\n                content = self.tool_manager.execute_tool(\"read_file\", file_path)\n                source_code[file_path] = content\n            except Exception as e:\n                logger.warning(f\"Could not read source file {file_path}: {e}\")\n                source_code[file_path] = f\"# Error reading file: {e}\"\n        \n        return source_code\n    \n    def get_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a summary of agent performance.\"\"\"\n        if not self.task_results:\n            return {\n                \"agent_id\": self.agent_id,\n                \"generation\": self.generation,\n                \"total_tasks\": 0,\n                \"success_rate\": 0.0,\n                \"average_score\": 0.0,\n                \"total_score\": 0.0\n            }\n        \n        return {\n            \"agent_id\": self.agent_id,\n            \"generation\": self.generation,\n            \"parent_id\": self.parent_id,\n            \"total_tasks\": len(self.task_results),\n            \"success_rate\": self.success_rate,\n            \"average_score\": self.total_score / len(self.task_results),\n            \"total_score\": self.total_score,\n            \"tools_available\": len(self.tool_manager.tools),\n            \"recent_performance\": self._analyze_recent_performance()\n        }\n    \n    def create_child_agent(self, mutation_result: Dict[str, Any]) -> 'PrometheusAgent':\n        \"\"\"Create a child agent after successful mutation.\"\"\"\n        child_id = f\"agent_{uuid.uuid4().hex[:8]}\"\n        child_agent = PrometheusAgent(child_id, self.project_root)\n        child_agent.generation = self.generation + 1\n        child_agent.parent_id = self.agent_id\n        \n        logger.info(f\"Created child agent {child_id} from parent {self.agent_id}\")\n        return child_agent\n\nif __name__ == \"__main__\":\n    # Test the agent\n    import tempfile\n    \n    with tempfile.TemporaryDirectory() as temp_dir:\n        # Create test agent\n        agent = PrometheusAgent(project_root=temp_dir)\n        \n        # Test task solving\n        test_task = {\n            \"instance_id\": \"test_task_1\",\n            \"problem_statement\": \"Write a function to calculate factorial\",\n            \"repo\": \"test_repo\",\n            \"base_commit\": \"abc123\"\n        }\n        \n        print(\"Testing task solving...\")\n        result = agent.solve_task(test_task)\n        print(f\"Task result: {result}\")\n        \n        # Test self-reflection\n        print(\"\\nTesting self-reflection...\")\n        source_code = agent.get_source_code()\n        performance_logs = \"Recent performance has been suboptimal\"\n        \n        improvement_json = agent.self_reflect_and_improve(source_code, performance_logs)\n        print(f\"Improvement suggestions: {improvement_json}\")\n        \n        # Show performance summary\n        print(\"\\nPerformance summary:\")\n        summary = agent.get_performance_summary()\n        for key, value in summary.items():\n            print(f\"  {key}: {value}\")\n",
    "agent/prompts.py": "\"\"\"Agent prompts and templates for Prometheus 2.0.\"\"\"\n\nfrom config import AGENT_SYSTEM_PROMPT, SELF_REFLECTION_PROMPT\n\n# Main system prompt for the agent\nSYSTEM_PROMPT = AGENT_SYSTEM_PROMPT\n\n# Self-reflection prompt template\nREFLECTION_PROMPT = SELF_REFLECTION_PROMPT\n\n# Problem-solving prompt template\nPROBLEM_SOLVING_PROMPT = \"\"\"You are tasked with solving the following software engineering problem:\n\nPROBLEM DESCRIPTION:\n{problem_statement}\n\nREPOSITORY: {repo_name}\nBASE COMMIT: {base_commit}\n\nAVAILABLE TOOLS:\n{available_tools}\n\nYour approach should be:\n1. Understand the problem thoroughly\n2. Use web_search to find relevant information if needed\n3. Examine the codebase using read_file and list_directory\n4. Develop a solution strategy\n5. Implement the solution by writing/modifying files\n6. Test your solution if possible\n\nThink step by step and explain your reasoning. Always use the available tools to gather information and implement your solution.\n\nRemember: You must generate actual code changes that solve the described problem.\"\"\"\n\n# Code analysis prompt\nCODE_ANALYSIS_PROMPT = \"\"\"Analyze the following code and identify potential improvements:\n\nCODE:\n{code}\n\nCONTEXT:\n- File: {file_path}\n- Purpose: {purpose}\n- Current issues: {issues}\n\nPlease provide:\n1. Code quality assessment\n2. Potential bugs or issues\n3. Performance improvements\n4. Best practice recommendations\n5. Specific code changes needed\n\nFocus on practical, implementable improvements.\"\"\"\n\n# Tool creation prompt\nTOOL_CREATION_PROMPT = \"\"\"You need to create a new tool to solve a specific problem.\n\nPROBLEM: {problem_description}\nEXISTING TOOLS: {existing_tools}\n\nDesign and implement a new tool that addresses this problem. The tool should:\n1. Have a clear, descriptive name\n2. Include proper documentation\n3. Handle errors gracefully\n4. Follow Python best practices\n5. Be reusable for similar problems\n\nProvide the complete tool implementation as Python code.\"\"\"\n\n# Performance improvement prompt\nIMPROVEMENT_PROMPT = \"\"\"Based on your recent performance data, identify specific areas for improvement:\n\nPERFORMANCE DATA:\n{performance_data}\n\nCURRENT APPROACH:\n{current_approach}\n\nKNOWN ISSUES:\n{known_issues}\n\nResearch and propose specific improvements to:\n1. Problem-solving methodology\n2. Code implementation strategies\n3. Tool usage efficiency\n4. Error handling and recovery\n\nUse web search to find better approaches if needed. Provide concrete, implementable changes.\"\"\"\n"
  },
  "metadata": {
    "shutdown_iteration": 1,
    "final_agent": true
  }
}